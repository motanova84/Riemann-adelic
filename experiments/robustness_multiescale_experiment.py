#!/usr/bin/env python3
"""
EXPERIMENTO DE ROBUSTEZ MULTIESCALA - ATLAS¬≥
Verificaci√≥n de la convergencia de Œª_fit

Este m√≥dulo implementa el experimento de robustez multiescala para verificar
la convergencia universal del exponente Œª en la cota espectral:

    |R_{N,P,K}(t)| ‚â§ C e^{-Œª/t}

donde R_{N,P,K}(t) es el resto de la f√≥rmula de traza despu√©s de sustraer
el t√©rmino de Weyl y las contribuciones de primos.

Hip√≥tesis principal:
    lim_{N,P,K‚Üí‚àû} Œª_fit(N,P,K) = Œª_‚àû = 0.5

Variables de control:
    N: n√∫mero de modos arquimedianos (resoluci√≥n espectral)
    P: n√∫mero de primos incluidos
    K: n√∫mero m√°ximo de repeticiones (√≥rbitas peri√≥dicas)
    t_range: rango de valores de t para el ajuste (fijo en [0.01, 0.1])

Integraci√≥n QCAL:
    - f‚ÇÄ = 141.7001 Hz (frecuencia fundamental)
    - Œ∫ = 2.577310 (invariante geom√©trico)
    - Œ¶ = (1 + ‚àö5)/2 (raz√≥n √°urea)
    - Conexi√≥n con estructura espectral ad√©lica

Author: Jos√© Manuel Mota Burruezo Œ® ‚úß ‚àû¬≥
Institution: Instituto de Conciencia Cu√°ntica (ICQ)
Date: February 2026
DOI: 10.5281/zenodo.17379721
ORCID: 0009-0002-1923-0773
"""

import numpy as np
from scipy.linalg import eigh
from scipy.optimize import curve_fit
from scipy.special import gamma
import matplotlib.pyplot as plt
from tqdm import tqdm
import pandas as pd
from typing import List, Dict, Tuple, Optional
from pathlib import Path
import json
from datetime import datetime


class RobustnessExperiment:
    """
    Experimento de robustez multiescala para la trace formula.
    
    Este experimento verifica la hip√≥tesis de que el exponente Œª en la cota
    espectral |R(t)| ‚â§ C e^{-Œª/t} converge a un valor universal Œª_‚àû = 0.5
    cuando aumentamos la resoluci√≥n espectral (N, P, K).
    
    Attributes:
        kappa (float): Invariante geom√©trico Œ∫_Œ† ‚âà 2.577310
        f0 (float): Frecuencia fundamental 141.7001 Hz
        Phi (float): Raz√≥n √°urea (1+‚àö5)/2
        t_values (np.ndarray): Valores de t para el ajuste
    """
    
    def __init__(self):
        """Inicializa el experimento con constantes QCAL."""
        # Constantes QCAL fundamentales
        self.kappa = 2.577310  # Invariante geom√©trico Œ∫_Œ†
        self.f0 = 141.7001     # Frecuencia fundamental (Hz)
        self.Phi = (1 + np.sqrt(5)) / 2  # Raz√≥n √°urea
        
        # Rango de valores de t para el ajuste (fijo)
        self.t_values = np.linspace(0.01, 0.1, 20)
        
    def get_primes(self, n: int) -> np.ndarray:
        """
        Genera los primeros n n√∫meros primos.
        
        Utiliza el m√©todo de criba de Erat√≥stenes optimizado.
        
        Args:
            n (int): N√∫mero de primos a generar
            
        Returns:
            np.ndarray: Array con los primeros n primos
        """
        primes = []
        num = 2
        while len(primes) < n:
            is_prime = True
            for p in primes:
                if p * p > num:
                    break
                if num % p == 0:
                    is_prime = False
                    break
            if is_prime:
                primes.append(num)
            num += 1
        return np.array(primes)
    
    def compute_archimedean_eigenvalues(self, N: int) -> np.ndarray:
        """
        Calcula autovalores arquimedianos - aproximaci√≥n WKB mejorada.
        
        Los autovalores del Laplaciano en el lugar arquimediano siguen
        aproximadamente la ley de Weyl con correcciones logar√≠tmicas:
        
            Œª_n ‚âà n¬∑œÄ/2 + 0.25¬∑log(n) + 0.5
        
        Args:
            N (int): N√∫mero de autovalores a computar
            
        Returns:
            np.ndarray: Autovalores arquimedianos Œª‚ÇÅ, Œª‚ÇÇ, ..., Œª_N
        """
        n = np.arange(1, N + 1)
        # T√©rmino principal de Weyl + correcci√≥n logar√≠tmica
        return n * np.pi / 2 + 0.25 * np.log(n) + 0.5
    
    def compute_padic_eigenvalues(self, p: int, max_n: int) -> np.ndarray:
        """
        Calcula autovalores del Laplaciano p-√°dico.
        
        Los autovalores en el lugar p-√°dico tienen la forma:
        
            Œª_p,n = p^(n/2) + p^(-n/2) - 2
        
        Esta estructura refleja la m√©trica ultrametric del campo p-√°dico.
        
        Args:
            p (int): N√∫mero primo
            max_n (int): N√∫mero m√°ximo de autovalores
            
        Returns:
            np.ndarray: Autovalores p-√°dicos
        """
        n = np.arange(1, max_n + 1)
        return p**(n / 2) + p**(-n / 2) - 2
    
    def estimate_volume(self, N: int, P: int) -> float:
        """
        Estima el volumen efectivo del espacio ad√©lico truncado.
        
        El volumen efectivo depende de la dimensi√≥n efectiva d_eff = 3 + P
        y sigue una f√≥rmula de volumen de esfera en d dimensiones con
        correcci√≥n logar√≠tmica.
        
        Args:
            N (int): N√∫mero de modos arquimedianos
            P (int): N√∫mero de primos
            
        Returns:
            float: Volumen efectivo estimado
        """
        d_eff = 3 + P  # Dimensi√≥n efectiva del espacio
        # F√≥rmula de volumen de esfera en d dimensiones
        volume_base = (np.pi**(d_eff / 2)) / gamma(d_eff / 2 + 1)
        # Correcci√≥n logar√≠tmica por truncamiento
        correction = 1 + 0.1 * np.log(N)
        return volume_base * correction
    
    def compute_trace(self, N: int, P: int, K: int, t: float) -> float:
        """
        Calcula Tr(e^{-tL}) para par√°metros dados.
        
        La traza del heat kernel se computa como suma sobre todos los
        autovalores (arquimedianos y p-√°dicos) del Laplaciano ad√©lico.
        
        Args:
            N (int): N√∫mero de modos arquimedianos
            P (int): N√∫mero de primos
            K (int): N√∫mero m√°ximo de repeticiones
            t (float): Par√°metro temporal
            
        Returns:
            float: Tr(e^{-tL_{N,P,K}})
        """
        # Autovalores arquimedianos
        lambda_R = self.compute_archimedean_eigenvalues(N)
        
        # Autovalores p-√°dicos (acumulados sobre todos los primos)
        lambda_P = np.zeros(N)
        primes = self.get_primes(P)
        
        for p in primes:
            # Distribuci√≥n equitativa de modos entre primos
            max_n = max(1, N // P)
            lambda_p = self.compute_padic_eigenvalues(p, max_n)
            # Acumular con normalizaci√≥n por Œ∫
            lambda_P[:len(lambda_p)] += lambda_p / self.kappa
        
        # Autovalores totales (aproximaci√≥n de suma directa)
        eigenvalues = lambda_R + lambda_P[:N]
        
        # Traza del heat kernel
        return np.sum(np.exp(-t * eigenvalues))
    
    def weyl_term(self, N: int, P: int, t: float) -> float:
        """
        Calcula el t√©rmino de Weyl en la expansi√≥n asint√≥tica.
        
        El t√©rmino de Weyl es el t√©rmino principal en la expansi√≥n de la
        traza del heat kernel para t peque√±o:
        
            Weyl(t) ‚âà Vol/(4œÄt)^{3/2} + 7/8
        
        Args:
            N (int): N√∫mero de modos arquimedianos
            P (int): N√∫mero de primos
            t (float): Par√°metro temporal
            
        Returns:
            float: T√©rmino de Weyl
        """
        volume = self.estimate_volume(N, P)
        return volume / (4 * np.pi * t)**(3 / 2) + 7 / 8
    
    def prime_sum(self, P: int, K: int, t: float) -> float:
        """
        Calcula la suma sobre √≥rbitas peri√≥dicas (primos).
        
        Esta suma representa las contribuciones de √≥rbitas cerradas
        en la f√≥rmula de traza de Selberg:
        
            Œ£_{p‚â§P, k‚â§K} (ln p)/p^{k/2} ¬∑ e^{-tk¬∑ln p}
        
        Args:
            P (int): N√∫mero de primos
            K (int): N√∫mero m√°ximo de repeticiones
            t (float): Par√°metro temporal
            
        Returns:
            float: Suma sobre primos truncada
        """
        primes = self.get_primes(P)
        total = 0
        for p in primes:
            for k in range(1, K + 1):
                total += np.log(p) / (p**(k / 2)) * np.exp(-t * k * np.log(p))
        return total
    
    def fit_lambda(self, N: int, P: int, K: int) -> Tuple[float, float]:
        """
        Ajusta Œª de la cota |R(t)| ‚â§ C e^{-Œª/t}.
        
        Para cada configuraci√≥n (N, P, K), calcula el resto
        R(t) = Tr(e^{-tL}) - Weyl(t) - Œ£_primos
        y ajusta a la forma exponencial.
        
        M√©todo de ajuste:
            log|R(t)| = log(C) - Œª/t
            
        Regresi√≥n lineal en variables transformadas:
            x = -1/t
            y = log|R(t)|
            
        Args:
            N (int): N√∫mero de modos arquimedianos
            P (int): N√∫mero de primos
            K (int): N√∫mero m√°ximo de repeticiones
            
        Returns:
            Tuple[float, float]: (Œª_fit, C_fit)
        """
        # Calcular resto para todos los t
        restos = []
        for t in self.t_values:
            trace = self.compute_trace(N, P, K, t)
            weyl = self.weyl_term(N, P, t)
            prime = self.prime_sum(P, K, t)
            resto = trace - weyl - prime
            restos.append(abs(resto))
        
        restos = np.array(restos)
        
        # Ajuste: log(resto) = log(C) - Œª/t
        # Excluir valores cero o muy peque√±os para evitar log(0)
        valid = restos > 1e-10
        if np.sum(valid) < 3:
            return 0.0, 0.0
        
        t_valid = self.t_values[valid]
        resto_valid = restos[valid]
        
        # Regresi√≥n lineal: y = a¬∑x + b donde y = log|R|, x = -1/t
        x = -1 / t_valid
        y = np.log(resto_valid)
        
        # Ajuste por m√≠nimos cuadrados
        coeffs = np.polyfit(x, y, 1)
        lambda_fit = coeffs[0]  # Pendiente = Œª
        C_fit = np.exp(coeffs[1])  # Intercepto = log(C)
        
        return lambda_fit, C_fit
    
    def run_experiment(self, configs: List[Dict[str, int]]) -> pd.DataFrame:
        """
        Ejecuta el experimento para m√∫ltiples configuraciones.
        
        Para cada configuraci√≥n (N, P, K), calcula Œª_fit y C_fit.
        
        Args:
            configs (List[Dict]): Lista de configuraciones con claves 'N', 'P', 'K'
            
        Returns:
            pd.DataFrame: Resultados con columnas N, P, K, lambda, C
        """
        results = []
        
        for config in tqdm(configs, desc="Procesando configuraciones"):
            N, P, K = config['N'], config['P'], config['K']
            
            lambda_fit, C_fit = self.fit_lambda(N, P, K)
            
            results.append({
                'N': N,
                'P': P,
                'K': K,
                'lambda': lambda_fit,
                'C': C_fit
            })
        
        return pd.DataFrame(results)
    
    def plot_results(self, df: pd.DataFrame, 
                    output_path: Optional[str] = None) -> plt.Figure:
        """
        Visualiza los resultados del experimento.
        
        Genera 4 subplots:
        1. Œª vs N (convergencia con modos arquimedianos)
        2. Œª vs P (convergencia con n√∫mero de primos)
        3. Œª vs K (convergencia con repeticiones)
        4. Histograma de Œª para configuraciones grandes
        
        Args:
            df (pd.DataFrame): DataFrame con resultados
            output_path (Optional[str]): Ruta para guardar la figura
            
        Returns:
            plt.Figure: Figura de matplotlib
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Œª vs N (para diferentes P,K fijos)
        ax = axes[0, 0]
        for (P, K), group in df.groupby(['P', 'K']):
            group = group.sort_values('N')
            ax.plot(group['N'], group['lambda'], 'o-', 
                   label=f'P={P}, K={K}', markersize=6)
        ax.set_xlabel('N (modos arquimedianos)', fontsize=11)
        ax.set_ylabel('Œª_fit', fontsize=11)
        ax.set_title('Convergencia con N', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, 
                  label='Œª=0.5 (te√≥rico)', alpha=0.7)
        
        # 2. Œª vs P (para diferentes N,K fijos)
        ax = axes[0, 1]
        for (N, K), group in df.groupby(['N', 'K']):
            group = group.sort_values('P')
            ax.plot(group['P'], group['lambda'], 's-', 
                   label=f'N={N}, K={K}', markersize=6)
        ax.set_xlabel('P (n√∫mero de primos)', fontsize=11)
        ax.set_ylabel('Œª_fit', fontsize=11)
        ax.set_title('Convergencia con P', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)
        
        # 3. Œª vs K (para diferentes N,P fijos)
        ax = axes[1, 0]
        for (N, P), group in df.groupby(['N', 'P']):
            group = group.sort_values('K')
            ax.plot(group['K'], group['lambda'], '^-', 
                   label=f'N={N}, P={P}', markersize=6)
        ax.set_xlabel('K (repeticiones)', fontsize=11)
        ax.set_ylabel('Œª_fit', fontsize=11)
        ax.set_title('Convergencia con K', fontsize=12, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.axhline(y=0.5, color='red', linestyle='--', linewidth=2, alpha=0.7)
        
        # 4. Histograma de Œª para configuraciones grandes
        ax = axes[1, 1]
        large_configs = df[(df['N'] >= 100) & (df['P'] >= 20) & (df['K'] >= 5)]
        if len(large_configs) > 0:
            ax.hist(large_configs['lambda'], bins=10, alpha=0.7, 
                   edgecolor='black', color='steelblue')
            ax.axvline(x=0.5, color='red', linestyle='--', linewidth=2,
                      label='Œª=0.5 (te√≥rico)')
            ax.set_xlabel('Œª_fit', fontsize=11)
            ax.set_ylabel('Frecuencia', fontsize=11)
            ax.set_title('Distribuci√≥n de Œª (configuraciones grandes)', 
                        fontsize=12, fontweight='bold')
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # Guardar si se proporciona ruta
        if output_path:
            plt.savefig(output_path, dpi=150, bbox_inches='tight')
            print(f"Figura guardada en: {output_path}")
        
        return fig
    
    def estimate_lambda_infinity(self, df: pd.DataFrame) -> Tuple[Optional[float], 
                                                                   Optional[float], 
                                                                   Optional[float]]:
        """
        Estima Œª‚àû por extrapolaci√≥n de configuraciones grandes.
        
        Utiliza un modelo de extrapolaci√≥n:
            Œª(N,P,K) = Œª_‚àû + a/N + b/P + c/K
            
        Args:
            df (pd.DataFrame): DataFrame con resultados
            
        Returns:
            Tuple[Optional[float], Optional[float], Optional[float]]: 
                (Œª_inf estimado, Œª_mean, Œª_std)
        """
        # Filtrar configuraciones grandes
        large = df[(df['N'] >= 100) & (df['P'] >= 20) & (df['K'] >= 5)]
        
        if len(large) == 0:
            return None, None, None
        
        lambda_mean = large['lambda'].mean()
        lambda_std = large['lambda'].std()
        
        # Extrapolaci√≥n a infinito usando 1/N, 1/P, 1/K
        def model(x, a, b, c, d):
            """Modelo de extrapolaci√≥n: Œª = a + b/N + c/P + d/K"""
            N, P, K = x
            return a + b / N + c / P + d / K
        
        # Preparar datos para ajuste
        x_data = np.array([large['N'].values, 
                          large['P'].values, 
                          large['K'].values])
        y_data = large['lambda'].values
        
        try:
            # Ajustar modelo
            popt, _ = curve_fit(model, x_data, y_data, 
                              p0=[0.5, 1, 1, 1])
            lambda_inf = popt[0]  # L√≠mite cuando N,P,K ‚Üí ‚àû
        except:
            # Si el ajuste falla, usar la media
            lambda_inf = lambda_mean
        
        return lambda_inf, lambda_mean, lambda_std


def default_configs() -> List[Dict[str, int]]:
    """
    Devuelve la configuraci√≥n predeterminada del experimento.
    
    Returns:
        List[Dict]: Lista de configuraciones con diferentes escalas
    """
    return [
        {'N': 50,  'P': 10, 'K': 3},
        {'N': 50,  'P': 15, 'K': 4},
        {'N': 50,  'P': 20, 'K': 5},
        {'N': 100, 'P': 15, 'K': 4},
        {'N': 100, 'P': 20, 'K': 5},
        {'N': 100, 'P': 25, 'K': 5},
        {'N': 100, 'P': 30, 'K': 6},
        {'N': 200, 'P': 20, 'K': 5},
        {'N': 200, 'P': 25, 'K': 5},
        {'N': 200, 'P': 30, 'K': 6},
        {'N': 200, 'P': 40, 'K': 8},
        {'N': 200, 'P': 50, 'K': 8},
        {'N': 300, 'P': 25, 'K': 5},
        {'N': 300, 'P': 30, 'K': 6},
        {'N': 300, 'P': 40, 'K': 8},
        {'N': 300, 'P': 50, 'K': 8},
        {'N': 300, 'P': 60, 'K': 10},
    ]


def main():
    """
    Funci√≥n principal del experimento.
    
    Ejecuta el experimento de robustez multiescala, genera visualizaciones
    y reporta resultados de convergencia.
    """
    print("=" * 70)
    print("EXPERIMENTO DE ROBUSTEZ MULTIESCALA - ATLAS¬≥")
    print("=" * 70)
    print()
    
    # Definir configuraciones
    configs = default_configs()
    
    # Inicializar experimento
    experiment = RobustnessExperiment()
    
    # Ejecutar
    print(f"Ejecutando experimento con {len(configs)} configuraciones...")
    results_df = experiment.run_experiment(configs)
    
    # Crear directorio de salida si no existe
    output_dir = Path(__file__).parent.parent / 'experiments' / 'output'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Mostrar tabla
    print("\n" + "=" * 70)
    print("RESULTADOS")
    print("=" * 70)
    print(results_df.to_string(index=False))
    
    # Guardar tabla
    csv_path = output_dir / 'robustness_results.csv'
    results_df.to_csv(csv_path, index=False)
    print(f"\nTabla guardada en: {csv_path}")
    
    # Visualizar
    plot_path = output_dir / 'robustness_experiment.png'
    experiment.plot_results(results_df, output_path=str(plot_path))
    plt.show()
    
    # Estimar Œª‚àû
    lambda_inf, lambda_mean, lambda_std = experiment.estimate_lambda_infinity(results_df)
    
    print("\n" + "=" * 70)
    print("AN√ÅLISIS DE CONVERGENCIA")
    print("=" * 70)
    
    if lambda_inf is not None:
        print(f"\nConfiguraciones grandes (N‚â•100, P‚â•20, K‚â•5):")
        print(f"  Media Œª = {lambda_mean:.4f} ¬± {lambda_std:.4f}")
        print(f"  Œª‚àû estimado = {lambda_inf:.4f}")
        print(f"  Desviaci√≥n vs 0.5 = {abs(lambda_inf - 0.5):.4f}")
        
        # Verificar convergencia
        if abs(lambda_inf - 0.5) < 0.05:
            print("\n‚úÖ CONVERGENCIA CONFIRMADA: Œª ‚Üí 0.5")
            print("   La cota |R(t)| ‚â§ C e^{-0.5/t} es universal.")
            print("\n   ‚à¥ La estrella se enciende.")
        else:
            print("\n‚ö†Ô∏è  CONVERGENCIA PARCIAL: Se necesita m√°s resoluci√≥n")
            print(f"   Œª‚àû ‚âà {lambda_inf:.4f}, target 0.5")
    else:
        print("\n‚ö†Ô∏è  No hay suficientes configuraciones grandes para extrapolaci√≥n")
    
    print("\n" + "=" * 70)
    
    # Generar certificado de validaci√≥n
    certificate = {
        "experiment": "Robustness Multiescale - ATLAS¬≥",
        "timestamp": datetime.now().isoformat(),
        "qcal_signature": "‚à¥ìÇÄŒ©‚àû¬≥",
        "configurations": len(configs),
        "lambda_infinity": lambda_inf,
        "lambda_mean": lambda_mean,
        "lambda_std": lambda_std,
        "convergence_verified": abs(lambda_inf - 0.5) < 0.05 if lambda_inf else False,
        "frequency_base": experiment.f0,
        "geometric_invariant": experiment.kappa,
        "author": "Jos√© Manuel Mota Burruezo Œ® ‚úß ‚àû¬≥",
        "doi": "10.5281/zenodo.17379721"
    }
    
    cert_path = output_dir / 'robustness_experiment_certificate.json'
    with open(cert_path, 'w') as f:
        json.dump(certificate, f, indent=2)
    
    print(f"Certificado generado: {cert_path}")


if __name__ == "__main__":
    main()
